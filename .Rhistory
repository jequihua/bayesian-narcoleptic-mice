net <- iamb(D)
}
else if (learn.method == 'mmpc')
{
net <- mmpc(D)
}
else if (learn.method == 'si.hiton.pc')
{
net <- si.hiton.pc(D)
}
else if (learn.method == 'rsmax2')
{
net <- rsmax2(D)
}
## learning the network weights
net.fit <- bn.fit(net, D, method='bayes', iss=1)
return(net.fit)
}
# function to plot graphs
plotD3bn <- function(bn) {
varNames <- nodes(bn)
# Nodes should be zero indexed!
links <- data.frame(arcs(bn)) %>%
mutate(from = match(from, varNames)-1, to = match(to, varNames)-1, value = 1)
nodes <- data.frame(name = varNames) %>%
mutate(group = 1, size = 30)
networkD3::forceNetwork(
Links = links,
Nodes = nodes,
Source = "from",
Target = "to",
Value = "value",
NodeID = "name",
Group = "group",
fontSize = 20,
zoom = TRUE,
arrows = TRUE,
bounded = TRUE,
opacityNoHover = 1,
linkDistance = 100
)
}
eval.bn <- function(bn, data){
n.instances <- dim(data)[1]
ll <- logLik(bn, data)
avg.ll <- ll / n.instances
return(avg.ll)
}
ll.bn <- function(bn, data){
ll <- logLik(bn, data, by.sample=TRUE)
return(ll)
}
learn.bn <- function(D, learn.method='mmhc'){
## learning the network structure
if (learn.method == 'mmhc')
{
net <- mmhc(D)
}
else if (learn.method == 'hc')
{
net <- hc(D, score='k2')
}
else if (learn.method == 'tabu')
{
net <- tabu(D)
}
else if (learn.method == 'gs')
{
net <- gs(D)
}
else if (learn.method == 'iamb')
{
net <- iamb(D)
}
else if (learn.method == 'mmpc')
{
net <- mmpc(D)
}
else if (learn.method == 'si.hiton.pc')
{
net <- si.hiton.pc(D)
}
else if (learn.method == 'rsmax2')
{
net <- rsmax2(D)
}
## learning the network weights
net.fit <- bn.fit(net, D, method='bayes', iss=1)
return(net.fit)
}
# load packages
library("raster")
library("rgdal")
library("bnlearn")
library("randomForest")
# Load species
sps = readRDS("D:/work4/sdm_felinos/data_final/puma_lynx_recent.rds")
# Species of interes
spi = sps[sps$especievalida=="Lynx rufus",]
# load pixel id raster
pixel_id = raster("D:/work4/sdm_felinos/layers_pixel_id/pixel_id.tif")
# species data to spatial
spi = spi[,c("longitud","latitud","especievalida","fechacolecta")]
coordinates(spi)=~longitud+latitud
projection(spi)=projection(pixel_id)
# extract id's
extract_id = extract(pixel_id,spi)
# as new variable in spi
spi$pixel_id = extract_id
# load rasters
raster_list = list.files("D:/work4/sdm_felinos/layers_final/",
pattern="\\.tif$",full.names = TRUE,recursive=TRUE)
# initialize brick
biobrik = brick()
for (i in 1:length(raster_list)){
rast = raster(raster_list[i])
rast[rast==-9999]=NA
biobrik=addLayer(biobrik,rast)
}
# add pixel id as band
biobrik = addLayer(biobrik,pixel_id)
# destroy duplicates
spi = spi[!duplicated(spi@data),]
spi = spi[!is.na(spi@data$pixel_id),]
# to huge table
biobrik_table = data.frame(rasterToPoints(biobrik))
# Discretize list of numeric valued variables
discretizeCols = function(bnbrik_df,
numeric_var_vec=3:13,
breaks_vec=rep(7,length(numeric_var_vec)),
method="interval"){
bnbrik_df[,numeric_var_vec] = bnlearn::discretize(bnbrik_df[,numeric_var_vec],
breaks=breaks_vec,
method=method)
return(bnbrik_df)
}
# discretize columns
biobrik_table_clean = biobrik_table[complete.cases(biobrik_table),]
biobrik_table_disc=discretizeCols(biobrik_table_clean)
head(biobrik_table_disc)
# clean biobrik
biobrik_clean = biobrik_table_disc[biobrik_table$pixel_id %in% spi@data$pixel_id,]
biobrik_clean = biobrik_clean[!is.na(biobrik_clean$pixel_id),]
head(biobrik_clean)
# Calculate loglik
vars = 4:13
biobrik_clean[,vars]
net = learn.bn(biobrik_clean[,vars], learn.method='hc')
loglik = ll.bn(net,biobrik_clean[,vars])
loglikp05 = quantile(loglik,0.05)
lls <- ll.bn(net, biobrik_table_disc[,vars])
lls_negative <- biobrik_table$pixel_id[lls<loglikp05]
idx = 1:length(lls_negative)
idx_sample = sample(idx,nrow(spi)*10)
lls_negative_sample = lls_negative[idx_sample]
lls_negative_sample = lls_negative_sample[!is.na(lls_negative_sample)]
sp_name="sp"
biobrik_table[,sp_name]=NA
biobrik_table[biobrik_table$pixel_id %in% spi$pixel_id,sp_name]=1
biobrik_table[biobrik_table$pixel_id %in% lls_negative_sample,sp_name]=0
### train random forest classifier
rftrain = biobrik_table[complete.cases(biobrik_table),]
rftrain$sp = as.factor(rftrain$sp)
ssize = floor(0.7*min(table(rftrain$sp)))
rf = randomForest(y=rftrain$sp,
x=rftrain[,vars],
ntree=1000,
importance=TRUE,
sampsize = c(ssize,ssize))
rf$confusion
predictiondf = biobrik_table[complete.cases(biobrik_table[,vars]),]
prediction = predict(rf,predictiondf[,vars],
type="prob")
vars
# Output map
head(prediction)
df = data.frame(x=predictiondf$x,
y=predictiondf$y,
suitability=prediction[,2])
coordinates(df)=~x+y
gridded(df)=TRUE
df=raster(df)
projection(df)=projection(pixel_id)
writeRaster(df, filename="D:/work4/sdm_felinos/final_maps/lynx_bn-rf_v1.tif", format="GTiff", overwrite=TRUE)
# load packages
library("raster")
library("rgdal")
library("bnlearn")
library("randomForest")
# Load species
sps = readRDS("D:/work4/sdm_felinos/data_final/puma_lynx_recent.rds")
# Species of interes
spi = sps[sps$especievalida=="Lynx rufus",]
# load pixel id raster
pixel_id = raster("D:/work4/sdm_felinos/layers_pixel_id/pixel_id.tif")
# species data to spatial
spi = spi[,c("longitud","latitud","especievalida","fechacolecta")]
coordinates(spi)=~longitud+latitud
projection(spi)=projection(pixel_id)
# extract id's
extract_id = extract(pixel_id,spi)
# as new variable in spi
spi$pixel_id = extract_id
# load rasters
raster_list = list.files("D:/work4/sdm_felinos/layers_final/",
pattern="\\.tif$",full.names = TRUE,recursive=TRUE)
# initialize brick
biobrik = brick()
for (i in 1:length(raster_list)){
rast = raster(raster_list[i])
rast[rast==-9999]=NA
biobrik=addLayer(biobrik,rast)
}
# add pixel id as band
biobrik = addLayer(biobrik,pixel_id)
# destroy duplicates
spi = spi[!duplicated(spi@data),]
spi = spi[!is.na(spi@data$pixel_id),]
# to huge table
biobrik_table = data.frame(rasterToPoints(biobrik))
# Discretize list of numeric valued variables
discretizeCols = function(bnbrik_df,
numeric_var_vec=3:12,
breaks_vec=rep(7,length(numeric_var_vec)),
method="interval"){
bnbrik_df[,numeric_var_vec] = bnlearn::discretize(bnbrik_df[,numeric_var_vec],
breaks=breaks_vec,
method=method)
return(bnbrik_df)
}
# discretize columns
biobrik_table_clean = biobrik_table[complete.cases(biobrik_table),]
biobrik_table_disc=discretizeCols(biobrik_table_clean)
head(biobrik_table_disc)
# clean biobrik
biobrik_clean = biobrik_table_disc[biobrik_table$pixel_id %in% spi@data$pixel_id,]
biobrik_clean = biobrik_clean[!is.na(biobrik_clean$pixel_id),]
head(biobrik_clean)
# Calculate loglik
vars = 4:12
biobrik_clean[,vars]
net = learn.bn(biobrik_clean[,vars], learn.method='hc')
loglik = ll.bn(net,biobrik_clean[,vars])
loglikp05 = quantile(loglik,0.05)
lls <- ll.bn(net, biobrik_table_disc[,vars])
lls_negative <- biobrik_table$pixel_id[lls<loglikp05]
idx = 1:length(lls_negative)
idx_sample = sample(idx,nrow(spi)*10)
lls_negative_sample = lls_negative[idx_sample]
lls_negative_sample = lls_negative_sample[!is.na(lls_negative_sample)]
sp_name="sp"
biobrik_table[,sp_name]=NA
biobrik_table[biobrik_table$pixel_id %in% spi$pixel_id,sp_name]=1
biobrik_table[biobrik_table$pixel_id %in% lls_negative_sample,sp_name]=0
### train random forest classifier
rftrain = biobrik_table[complete.cases(biobrik_table),]
rftrain$sp = as.factor(rftrain$sp)
ssize = floor(0.7*min(table(rftrain$sp)))
rf = randomForest(y=rftrain$sp,
x=rftrain[,vars],
ntree=1000,
importance=TRUE,
sampsize = c(ssize,ssize))
rf$confusion
predictiondf = biobrik_table[complete.cases(biobrik_table[,vars]),]
prediction = predict(rf,predictiondf[,vars],
type="prob")
vars
# Output map
head(prediction)
df = data.frame(x=predictiondf$x,
y=predictiondf$y,
suitability=prediction[,2])
coordinates(df)=~x+y
gridded(df)=TRUE
df=raster(df)
projection(df)=projection(pixel_id)
writeRaster(df, filename="D:/work4/sdm_felinos/final_maps/lynx_bn-rf_v1.tif", format="GTiff", overwrite=TRUE)
varImpPlot(rf)
library("readxl")
# Load data
cockroaches = read_excel("D:/work4/martin_phd/hierarchy_data_examples.xlsx",
"Cockroaches Dyadic")
head(cockroaches)
# Fit model
fit = brm(Data ~ (1|Winner) + (1|Loser),
data = cockroaches, family = poisson,
control = list(adapt_delta = 0.9999,max_treedepth=15))
# Load packages
library("brms")
# Fit model
fit = brm(Data ~ (1|Winner) + (1|Loser),
data = cockroaches, family = poisson,
control = list(adapt_delta = 0.9999,max_treedepth=15))
summary(fit)
# Fit model
fit = brm(Data ~ Winner + Loser,
data = cockroaches, family = poisson,
control = list(adapt_delta = 0.9999,max_treedepth=15))
# Fit model
fit = brm(Data ~ Winner + Loser,
data = cockroaches, family = poisson,
control = list(adapt_delta = 0.9999,max_treedepth=15))
summary(fit)
# Fit model
fit = brm(Data ~ Winner * Loser,
data = cockroaches, family = poisson,
control = list(adapt_delta = 0.9999,max_treedepth=15))
summary(fit)
# Fit model
fit = brm(Data ~ Winner + Loser,
data = cockroaches, family = poisson,
control = list(adapt_delta = 0.9999,max_treedepth=15))
setwd("D:/repositories/bayesian-narcoleptic-mice/")
# Load packages.
library("tidyverse")
library("ggplot2")
library("brms")
library("brmstools")
library("bayesplot")
library("emmeans")
library("tidybayes")
# Load misc. functions.
source("./R/misc_functions.R")
# Load recorded behaviors data.
data <- read_csv("./data/videos/recorded_behaviors.csv")
### Duration of each BA (narcoleptic attack) model.
ba_seconds <- filter(data,behavior=="BA")
# Change variable types.
ba_seconds$group <- as.factor(ba_seconds$group)
ba_seconds$mouse <- as.factor(ba_seconds$mouse)
# Which priors can we specify for our model?
get_prior(seconds ~ group + (1|mouse), data=ba_seconds)
# Fit brms model for duration of each BA attack.
ba_seconds_fit <- brm(seconds ~ group + (1|mouse),
data = ba_seconds,
family = gaussian,
control = list(adapt_delta = 0.999))
summary(ba_seconds_fit)
# Poisson regression offset log(time)
# Table available number of hours-data per mice.
mice_totalhours <- group_by(data,mouse) %>%
summarise(totaltime = sum(seconds)/3600)
# Join data and mice_hours.
dataa <- right_join(data,mice_totalhours,by="mouse")
ba_counts <- dataa %>%
mutate(mouse=paste0(group," ",mouse)) %>%
group_by(mouse) %>%
summarise(BA_counts = sum(behavior == "BA"),hours=min(totaltime)) %>%
separate(mouse,c("group","mouse"))
# Change variable types.
ba_counts$group <- as.factor(ba_counts$group)
ba_counts$mouse <- as.factor(ba_counts$mouse)
# Which priors can we specify for our model?
get_prior(BA_counts ~ group + (1|mouse)+offset(log(hours)), data=ba_counts)
# Fit brms model for absolute number of BA attacks per hour.
ba_counts_fit <- brm(BA_counts ~ group + offset(log(hours)),
data = ba_counts, family = poisson(), control = list(adapt_delta = 0.9999,max_treedepth=15))
# Chart raw data boxplots.
ggplot(ba_counts, aes(x=group, y=BA_counts)) + geom_boxplot(lwd=1.2)+
labs(x = NULL, y = "counts")+
ggtitle("BA counts")+
theme_bw(base_size = 40)+
xlab("Groups")
ggsave("./images4/2_ba_counts/1_ba_counts_rawdata_boxplots.png",
width = 10, height = 8, dpi = 500,device="png")
# Summary stats.
ba_counts_grouped = group_by(ba_counts,group)
ba_counts_summary = summarise(ba_counts_grouped,
n=n(),
n_mouse=n_distinct(mouse),
mean=mean(BA_counts),
median = median(BA_counts),
min=min(BA_counts),
max=max(BA_counts),
sd=sd(BA_counts),
Q0.25=quantile(BA_counts,0.25),
Q0.75=quantile(BA_counts,0.75),
IQR=IQR(BA_counts))
write_csv(ba_counts_summary,"./images4/2_ba_counts/1_ba_counts_rawdata_summary.csv")
# Coefficient plot.
coeff_plot_data = interval_data(ba_counts_fit)
plot_intervals(coeff_plot_data)
ggsave("./images4/2_ba_counts/2_ba_counts_modelfit_coefficientplot.png",
width = 10, height = 8, dpi = 500,device="png")
# Summary of model fit.
fit_summary = lazerhawk::brms_SummaryTable(ba_counts_fit,astrology=TRUE)
write_csv(fit_summary,"./images4/2_ba_counts/2_ba_counts_modelfit_fitsummary.csv")
credible_intervals_dat = credible_intervals_data_hours(ba_counts,ba_counts_fit)
write_csv(credible_intervals_dat,"./images4/2_ba_counts/3_ba_counts_modelinference_pairwisecomp_crudetable.csv")
plot_contrasts(credible_intervals_dat,xlab="Effect size")
ggsave("./images4/2_ba_counts/3_ba_duration_modelinference_pairwisecomp_crude.png",
width = 10, height = 8, dpi = 500,device="png")
# Evaluate model fit.
summary(ba_counts_fit)
# Total hours of sampling by mouse.
mice_totalseconds <- group_by(data,mouse) %>%
summarise(totaltime = sum(seconds))
# Join data and mice_hours.
dataa <- right_join(data,mice_totalseconds,by="mouse")
ba_percentage <- filter(dataa,behavior=="BA") %>%
mutate(mouse=paste0(group," ",mouse)) %>%
group_by(mouse) %>%
summarise(BA_totaltime = sum(seconds),hours=min(totaltime)) %>%
mutate(percentage_time_BA=(BA_totaltime/hours)) %>%
separate(mouse,c("group","mouse"))
# Change variable types.
ba_percentage$group <- as.factor(ba_percentage$group)
ba_percentage$mouse <- as.factor(ba_percentage$mouse)
# Which priors can we specify for our model?
get_prior(percentage_time_BA ~ group + (1|mouse), data=ba_percentage)
# Fit brms model for total time spent in BA state.
ba_totaltime_fit <- brm(percentage_time_BA ~ group,
data = ba_percentage, family = Beta, control = list(adapt_delta = 0.999,max_treedepth = 15))
summary(ba_totaltime_fit)
# Chart raw data boxplots.
ggplot(ba_percentage, aes(x=group, y=percentage_time_BA)) + geom_boxplot(lwd=1.2)+
labs(x = NULL, y = "% of time")+
ggtitle("% of time spent in BA")+
theme_bw(base_size = 40)+
xlab("Groups")
ggsave("./images4/3_ba_percentage/1_ba_counts_rawdata_boxplots.png",
width = 10, height = 8, dpi = 500,device="png")
# Summary stats.
ba_percentage_grouped = group_by(ba_percentage,group)
ba_percentage_summary = summarise(ba_percentage_grouped,
n=n(),
n_mouse=n_distinct(mouse),
mean=mean(percentage_time_BA),
median = median(percentage_time_BA),
min=min(percentage_time_BA),
max=max(percentage_time_BA),
sd=sd(percentage_time_BA),
Q0.25=quantile(percentage_time_BA,0.25),
Q0.75=quantile(percentage_time_BA,0.75),
IQR=IQR(percentage_time_BA))
write_csv(ba_percentage_summary,"./images4/3_ba_percentage/1_ba_percentage_rawdata_summary.csv")
ba_percentage_fit = ba_totaltime_fit
# Coefficient plot.
coeff_plot_data = interval_data(ba_percentage_fit)
plot_intervals(coeff_plot_data)
ggsave("./images4/3_ba_percentage/2_ba_percentage_modelfit_coefficientplot.png",
width = 10, height = 8, dpi = 500,device="png")
# Summary of model fit.
fit_summary = lazerhawk::brms_SummaryTable(ba_percentage_fit,astrology=TRUE)
write_csv(fit_summary,"./images4/3_ba_percentage/2_ba_percentage_modelfit_fitsummary.csv")
credible_intervals_dat = credible_intervals_data(ba_percentage,ba_percentage_fit)
write_csv(credible_intervals_dat,"./images4/3_ba_percentage/3_ba_percentage_modelinference_pairwisecomp_crudetable.csv")
plot_contrasts(credible_intervals_dat,xlab="Effect size")
ggsave("./images4/3_ba_percentage/3_ba_percentage_modelinference_pairwisecomp_crude.png",
width = 10, height = 8, dpi = 500,device="png")
coef(ba_totaltime_fit)
fixef(ba_totaltime_fit)
summary(ba_seconds_fit)
plot(ba_seconds_fit)
pp <- brms::pp_check(ba_seconds_fit,nsamples=10)
pp + theme_bw()
# Fit brms model for duration of each BA attack.
ba_seconds_fit <- brm(seconds ~ group + (1|mouse),
data = ba_seconds,
family = log-normal,
control = list(adapt_delta = 0.999))
# Fit brms model for duration of each BA attack.
ba_seconds_fit <- brm(seconds ~ group + (1|mouse),
data = ba_seconds,
family = lognormal,
control = list(adapt_delta = 0.999))
summary(ba_seconds_fit)
plot(ba_seconds_fit)
pp <- brms::pp_check(ba_seconds_fit,nsamples=10)
pp + theme_bw()
# Predictive posterior checks.
pp <- brms::pp_check(ba_seconds_fit,nsamples=20)
pp + theme_bw()
summary(ba_seconds_fit)
plot(ba_seconds_fit)
# Chart raw data boxplots.
ggplot(ba_seconds, aes(x=group, y=seconds)) + geom_boxplot(lwd=1.2)+
labs(x = NULL, y = "Seconds")+
ggtitle("Duration of each BA")+
theme_bw(base_size = 40)+
xlab("Groups")
ggsave("./images4/1_ba_duration/1_ba_duration_rawdata_boxplots.png",
width = 10, height = 8, dpi = 500,device="png")
# Summary stats.
ba_seconds_grouped = group_by(ba_seconds,group)
ba_seconds_summary = summarise(ba_seconds_grouped,
n=n(),
n_mouse=n_distinct(mouse),
mean=mean(seconds),
median = median(seconds),
min=min(seconds),
max=max(seconds),
sd=sd(seconds),
Q0.25=quantile(seconds,0.25),
Q0.75=quantile(seconds,0.75),
IQR=IQR(seconds))
write_csv(ba_seconds_summary,"./images4/1_ba_duration/1_ba_duration_rawdata_summary.csv")
# Coefficient plot.
coeff_plot_data = interval_data(ba_seconds_fit)
plot_intervals(coeff_plot_data)
ggsave("./images4/1_ba_duration/2_ba_duration_modelfit_coefficientplot.png",
width = 10, height = 8, dpi = 500,device="png")
# Summary of model fit.
fit_summary = lazerhawk::brms_SummaryTable(ba_seconds_fit,astrology=TRUE)
write_csv(fit_summary,"./images4/1_ba_duration/2_ba_duration_modelfit_fitsummary.csv")
# Contrasts
credible_intervals_dat = credible_intervals_data(ba_seconds,ba_seconds_fit)
write_csv(credible_intervals_dat,"./images4/1_ba_duration/3_ba_duration_modelinference_pairwisecomp_crudetable.csv")
plot_contrasts(credible_intervals_dat,xlab="Effect size")
ggsave("./images4/1_ba_duration/3_ba_duration_modelinference_pairwisecomp_crude.png",
width = 10, height = 8, dpi = 500,device="png")
